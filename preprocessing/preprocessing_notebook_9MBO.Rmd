Run Alexes notebook for 9 MO full surface AB panel to create dataframe of expression values
a

```{r,include=FALSE}
# setup the enviroment
# multiple packages may need to be installed

require("flowCore") #Used for reading the data
require("ggplot2")
require("ggridges") #visualization
require("stringr") #set of functions to manipulate strings type in order to have nice titles
require("rlist") #set of functions that allows to easely manipulate lists
require("reshape2") #visualization
require("flowStats") #Alignment functions
require("scales") #scale colour intensity for visualization
require("dplyr")

#libraries
library("flowCore")

# installations
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("flowCore")

if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("flowStats")



```


Functions to be called later - run code chunk to have the function 'callable' in your working enviromnent

```{r}
# functions Alex wrote
# will need to make these into a separate function with documentation

# function plotdensity_flow set

plotdensity_flowset <- function(flowset){ ggplot(melt(lapply(as.list(flowset@frames),function(x){x=as.data.frame(x@exprs)})), aes(x=value,y=L1,fill=L1)) + geom_density_ridges(alpha=.4,verbose=FALSE) +facet_wrap(~variable)+theme_light()} 

#defines a function for visualizing flowset with densityplots

# function renmame markers
# fcs files will have the marker names input during acquistion using flowjo

rename_markers<-function(flowset){#Defines a function to use marker names 
  copy_flowset=flowset[seq(along=flowset)]
  for (i in 1:length(copy_flowset)){
    marker.names=copy_flowset[[i]]@parameters@data$desc
    marker.names=lapply(marker.names,function(x){str_replace_all(x,"-","_")})
    colnames(copy_flowset[[i]]@exprs)<-unlist(lapply(marker.names, function(x){sapply(str_split(x,"_"),head,1)})) 
  }
  return(copy_flowset)
}



```


#Set user variables - This chunck allows the user to set the absolute path of the input data and the desired output folder.

--The input path has to contain only FCS files with or without an annotation file containing metadata. 

--The output path will be used for all the outputs (graphs and tables). The variable write_fcs_files has to be set to TRUE for writing fcs files at each step of the workflow (transformed and aligned), a folder for each step is created inside the output folder.

```{r}
#input_path <- "/home/alx/Documents/McGill_Neuro/Datasets/FACS_Paper/cell_line/" #Input folder containing multiple fcs files
input_path <- "/Users/rhalenathomas/Documents/Data/FlowCytometry/PhenoID/FlowDataFiles/9MBO"
#output_path <- "/home/alx/Documents/McGill_Neuro/Datasets/FACS_Paper/output_test/" #Output path 
output_path <- "/Users/rhalenathomas/Documents/Data/FlowCytometry/PhenoID/Analysis/9MBO/prepro_outs"


write_fcs_files <- TRUE #Set to true to write fcs files at each step (subsampled, transformed, aligned and scaled) - recommanded 

#Create output folder if it's not already created
if(dir.exists(output_path)==FALSE){ #check
  dir.create(output_path) #create directory
}


```


#Read in the data - This chunck reads in the data and selects the area values that are used for analysis. It creates a flowSet object (containing multiple flowFrames, corresponding to fcs files)

```{r}


# all the files in the input folder will be added to this data object flowset
flowset <- read.flowSet(path=input_path,transformation = FALSE ,emptyValue = FALSE,truncate_max_range = FALSE, package="flowCore") #Create a flowset object


flowset <- fsApply(flowset,function(x){x[ ,grepl("^[{'FJComp'}|{'FCS'}|{'SSC'}].*A$",colnames(flowset))]}) #Apply a function to flowSet (fsApply) that selects only the columns corresponding to areas values using "regular expression".

# save number of cell per sample (experiment) if desired
write.table(fsApply(flowset,function(x){dim(x@exprs)})[,1],file=paste0(output_path,"number_cells.csv"))#Counts the number of cells inside each flowframe and writes it into the output folder.

# see number of cells per sample
print(fsApply(flowset,function(x){dim(x@exprs)})[,1])


write.table(apply(as.data.frame(list.rbind(lapply(sampleNames(flowset),function(name){flowset[[name]]@description}))),2,as.character),file=paste0(output_path,"flowset_description.csv"))#Writes descriptions of each flowframe in a csv file

sampleNames(flowset) #Prints the name of each flowframe inside the flowset. User can modify it in the following chunck
#write.flowSet(flowset,outdir=paste0(output_path,"input_flowset"))#Writes the input flowset
#flowset=read.flowSet(path=paste0(output_path,"input_flowset"),phenoData = "annotation.txt")#Read the written flowset
```

#This chunck can be used for renaming the fcs files inside the flowset
```{r}
# create a vector with the new file names and assign them
# this must be a flowCore function
# note there are two ages - I'll look up the date and calulate ages later for old I'll write A and young B 0306 and 0317 are march 6 and march 17 the experiment dates of dissociation and acquistion

sampleNames(flowset) <- c("3450_0306","AIW002_0306","AJG001C_0306","3450_0317A","AIW002_0317A","AJG001C_0317A","3450_0317B","AIW002_0317B","AJG001C_0317B") #directly modify the names inside the flowset
sampleNames(flowset)
# now the sample or file names are changed under the 'frame' data slot


# this appears to be again reading in the data an getting the expression values
# I'll make this a function for the package

# call this function 'make_flowset'
# currently it is called read_reg 
# seems to combine the two reading in functions above

read_reg <- function(input_path,regular_expression,desc=FALSE) {
  flowset <- read.flowSet(path=input_path,transformation = FALSE ,emptyValue = FALSE,package="flowCore") #Create a flowset object
  if (desc==FALSE){ #Create a flowset object
    flowset <- fsApply(flowset,function(x){
      x[ ,grepl(regular_expression,colnames(flowset))]
      })
  }
  else{
    flowset <- fsApply(flowset,function(x){
      x[ ,which(grepl(regular_expression,x@parameters$desc))]
      })
  }
  return(flowset)
}


```


#Subsets - Allows to subset each fcs with a desired number of cells selected randomly (set the seed), go directly to transformation step for not subsetting

To determine if I want to subset cells or how many to subset I need to know the number of cells in each sample. This can be seen in the table above

For this analysis I'll take 9000 events for each sample except the final sample I'll take all 1578


Set an arbitrary number of cells for subsetting samples (if a fcs files has less cells it will be completely selected)
```{r,include=FALSE}
desired_size <- 9000
# I've run this chunk setting the cell number to 9000
```
Set the number to the smallest fcs files (containing the smallest amount of cells)
```{r,include=FALSE}
desired_size <- min(fsApply(flowset,function(x){nrow(x@exprs)}))
# not run
```
Subsets the data
```{r,echo=FALSE}
set.seed(42) #Set a seed for reproducibility 
sf <- sampleFilter(filterId = "SizeFilter", size =desired_size) #Creates a "filter" object to subset
flowset <- fsApply(flowset,function(x){Subset(x,sf,truncate_max_range = FALSE)}) #apply the filter on every flowframe

if(write_fcs_files==TRUE){
  write.flowSet(flowset,outdir=paste0(output_path,"subsample_input_flowset"))#Writes the subsample input flowset
  flowset=read.flowSet(path=paste0(output_path,"subsample_input_flowset"),phenoData = "annotation.txt")#Read the written flowset // The reason is for the phenodata management in the following steps
}
```

#Transformation -  Apply a mathematical function to make the data interpretable.
See [Finak G, Perez JM, Weng A, Gottardo R. Optimizing transformations for automated, high throughput analysis of flow cytometry data. BMC Bioinformatics. 2010;11:546. Published 2010 Nov 4. doi:10.1186/1471-2105-11-546] for more informations about transformations and possible optimizations.

See https://rdrr.io/bioc/flowCore/man/ for informations about available transformations in flowCore (used in this workflow)

Note flowjo displays data in the biexponential transformation as a default in histograms

Biexponential transformation
```{r,echo=FALSE}
inversebiexponentialTransform <- function(flowset,a = 0.5, b = 1, c = 0.5, d = 1, f = 0, w = 0){
  copy_flowset=flowset[seq(along=flowset)] #makes a copy of the input flowset
  for (i in 1:length(copy_flowset)){ #loop though index to get each flowframe
    copy_flowset[[i]]@exprs=a*exp(b*(copy_flowset[[i]]@exprs-w))-c*exp(-d*(copy_flowset[[i]]@exprs-w))+f
  }
  return(copy_flowset)
}

biexp  <- biexponentialTransform("biexp transform",a = 0.5, b = 1, c = 0.5, d = 1, f = 0, w = 0) #creates the transformation object (the values make it equivalent to arcsinh transform)
transformed_flowset <- transform(flowset, transformList(colnames(flowset), biexp)) #Apply the transformation

if(write_fcs_files==TRUE){#Check if user set the conditional for writing several fcs files
  write.flowSet(transformed_flowset,outdir=paste0(output_path,"transformed_flowset"))#writes the flowset
  transformed_flowset=read.flowSet(path=paste0(output_path,"transformed_flowset"),phenoData = "annotation.txt")
}
```


Scaling looses informations about the data and makes it harder to align. The main interest of this step is to harmonize the data for the classifier and make it comparable from one experience to another, thus I suggest to apply this step after the clustering.
We could potentially makes it after the alignment but since every flow frame (.fcs) is different from each other we would loose the alignment. (it would be interesting to try to normalize each marker considering all experiments at the same time)

Moreover the actual presence of some outliers (supposively from a bad cleaning of the data in the first place) biase the results concerning the z-score normalization.

#Alignment

Alignment of organoids
```{r}
normtr=gaussNorm(transformed_flowset,colnames(transformed_flowset)[c(3,5:6,9:length(colnames(transformed_flowset)))],max.lms = 2,peak.density.thr = 0.01) #Detects and align 2 peaks on the marker 3,5,6,9...14. 
expbe_norm2=normtr$flowset
normtr=gaussNorm(expbe_norm2,colnames(expbe_norm2)[c(4,7:8)],max.lms = 1,peak.density.thr = 0.05)#Detects and align 1 peak 
aligned_transformed_flowset=normtr$flowset
retrotransformed_flowset <- inversebiexponentialTransform(aligned_transformed_flowset) #apply the function for cancelling biexp transform 
```

Be carefull with this step. Check the density plots before and after the alignment to make sure peaks are aligned
```{r}
normtr <- gaussNorm(flowset = transformed_flowset,channel.names = colnames(transformed_flowset)[c(3:length(colnames(transformed_flowset)))], max.lms = 2, peak.density.thr = 0.01, peak.distance.thr = 0.5) #Detects and align 2 peaks (max.lms) for all markers except FSC and SSC (columns 3 to number of markers) the threshold are data-dependant and may have to differ from one analysis to another.

aligned_transformed_flowset <- normtr$flowset #Extract the flowset from the result of the alignment
retrotransformed_flowset <- inversebiexponentialTransform(aligned_transformed_flowset) #apply the function for cancelling biexp transform
```



```{r}
plotdensity_flowset(aligned_transformed_flowset)

plotdensity_flowset(transformed_flowset)
```


Densityplots for showing effects of transformations and alignment
```{r}
plotdensity_flowset(rename_markers(flowset))
plotdensity_flowset(rename_markers(transformed_flowset))
plotdensity_flowset(rename_markers(aligned_transformed_flowset))
plotdensity_flowset(rename_markers(retrotransformed_flowset))
```


#Writing csv files with informations

```{r}
# watchout -I'm renaming all the columns to match the first dataframe in the list to avoid the R bind error
# they were all the same by visual instruction but it's possible I'm renaming with the wrong AB which will be a huge problem

flowset_to_csv=function(flowset){  
  list_of_flowframes=fsApply(rename_markers(flowset),function(x){as.data.frame(x@exprs)})#Makes a list of dataframes
  list_names=names(list_of_flowframes)#extract names for not calling names() function at each loop
  for (index in seq_along(list_of_flowframes)){ #Iterates along the index for adding sample names
    list_of_flowframes[[index]]=list_of_flowframes[[index]] %>% #Using tidyverse package for adding features (name of batch)
      mutate(Batch=list_names[index])#Using tidyverse package for adding features (name of batch)
  #rename the columns to fix the rbind error 
      colnames(list_of_flowframes[[index]]) = colnames(list_of_flowframes[[1]])  
  }
  # this is wehre the error occurs but all the df have the same column names???
  ds=list.rbind(list_of_flowframes)#binds every fcs file in a single dataframe
  ds$cell=as.factor(unlist(lapply(as.list(c(1:length(flowset))),function(x){c(1:nrow(flowset[[x]]@exprs))})))#add cell IDs - cell count per sample 
  write.csv(ds,file=paste0(output_path,deparse(substitute(flowset)),".csv"))#save the R data for further usage
}

# there is an error in the appove function or the below application
# this should make a csv with the antibody names - I haven't renamed the antibodies and I don't see that code chunk 



flowset_to_csv(flowset)#apply the function
flowset_to_csv(transformed_flowset)#apply the function
flowset_to_csv(aligned_transformed_flowset)#apply the function
flowset_to_csv(retrotransformed_flowset)#apply the function


#Writes a text file with more informations about the system
sys=Sys.info() #store informations about the system
fileConn<-file(paste0(output_path,"call_description.txt")) #Writes txt files that describe the work 
writeLines(c(paste0(as.character(Sys.time())," ",Sys.timezone()),
            paste0("Import ", "8"," fcs files :"),
            "",
            sampleNames(flowset),
            "",
            paste0("data is subset to ",as.character(desired_size)," cells per fcs file"),
            "",
            paste0("written in ", output_path),
            "",
            "system informations:",
            paste0("sysname : ",sys[1]),
            paste0("release : ",sys[2]),
            paste0("version : ",sys[3]),
            paste0("node name : ",sys[4]),
            paste0("machine : ",sys[5]),
            paste0("login : ",sys[6]),
            paste0("user : ",sys[7]),
            paste0("effective_user : ",sys[8])
            ),
          fileConn)
close(fileConn)



# save an R object
# saveRDS(flowset, "/Users/rhalenathomas/Documents/Data/FlowCytometry/PhenoID/Analysis/9MBO/prepro_outs/flowset9MBO9000cells.RDS")


```

/!\ I kept normalization steps (scaling) in the notebook but I don't recommand to use it on cell lines. 

#Scaling - outlayers make the zscore normalization messy, it is not the case on "clean" dataset such as organoids. 
```{r}
zscorenorm_flowset <- function(flowset){ #Defines a function for applying zscore normalization to each flowframe within the flowset in parameter
  copy_flowset=flowset[seq(along=flowset)] #makes a copy of the input flowset
  for (i in 1:length(copy_flowset)){ #loop though index to get each flowframe
    copy_flowset[[i]]@exprs=scale(flowset[[i]]@exprs,center=TRUE,scale=TRUE)#zscore using the default scale function
  }
  return(copy_flowset)#Return the scaled flowset
}

minmax_flowset <- function(flowset){ #Defines a function for applying zscore normalization to each flowframe within the flowset in parameter
  copy_flowset=flowset[seq(along=flowset)] #makes a copy of the input flowset
  for (i in 1:length(copy_flowset)){ #loop though index to get each flowframe
    copy_flowset[[i]]@exprs=apply(flowset[[i]]@exprs,2,function(x){(x-min(x))/(max(x)-min(x))})#minmax normalization
  }
  return(copy_flowset)#Return the scaled flowset
}

scaledminmax_transformed_flowset <- minmax_flowset(aligned_transformed_flowset)
scaledzscore_transformed_flowset <- zscorenorm_flowset(transformed_flowset)

plotdensity_flowset(scaledminmax_transformed_flowset)
plotdensity_flowset(scaledzscore_transformed_flowset)
```


#This chunck allows the user to write in csv mean values of a given flowset per marker
```{r}
list_of_flowframes=fsApply(rename_markers(transformed_flowset),function(x){as.data.frame(x@exprs[,-c(1,2,9)])})#Makes a list of dataframes without FSC SSC and LiveDead markers
mean_values=as.data.frame(lapply(list_of_flowframes,function(x){apply(x,2,function(y){mean(y)})}))#compute the mean by marker within each element of the list of flowframes (dataframes)
write.csv(mean_values,file="/Users/rhalenathomas/Documents/Data/FlowCytometry/PhenoID/Analysis/9MBO/prepro_outs/meanvaluepermarker.csv")
```

#Correlation analysis

```{r}
# read in the reference matrix
expected_val=read.csv(file="/Users/rhalenathomas/Documents/Data/FlowCytometry/PhenoID/ReferenceMatrix_meanof6_20210517_B.csv") 
head(expected_val)
dim(expected_val) # cell types by rows, columns by 
#Format expected values to fit the old one
expected_val=t(expected_val) #transpose the dataframe for fitting the previous way of making correlation
# 
colnames(expected_val) <- expected_val[1,]#give column names the right names

expected_values=apply(as.data.frame(expected_val[-1,]),2, function(x){as.numeric(as.character(x))}) #change character to numeric values in a tortured way
rownames(expected_values)=rownames(expected_val[-1,]) #right rownames
expected_values=expected_values[,colnames(expected_values)!="Pericyte" & colnames(expected_values) != "Microglial"] #get rid of microglia and pericytes
```

```{r}

output_path="/home/alx/Documents/McGill_Neuro/Scripts/PhenoID/output_june/"
#2 functions to help
minmax_normalize <- function(x, na.rm=TRUE){return((x- min(x[!is.na(x)])) /(max(x[!is.na(x)])-min(x[!is.na(x)])))}
expected_values <- minmax_normalize(scale(expected_values)) #apply zscore normalization and minmax normalization (fit results between 0 and 1)

compute_corr<-function(cell_profile,celltypes_profile){
  celltypes=celltypes_profile[,c(1:ncol(celltypes_profile))]#2 because the first column of the given file is expected to be "annotation"
  res=as.vector(apply(celltypes,2,function(x){
    cor(cell_profile[which(!is.na(x))],x[which(!is.na(x))])}))#If a marker is "na" it won't be considered for computing the correlation coef
  names(res)<-colnames(celltypes)
  return(res)
}

fs=rename_markers(aligned_transformed_flowset) #Nice marker names

facsdata=list.rbind(lapply(as.list(fs@frames),function(x){x=as.data.frame(x@exprs)})) #I had to remove the live-dead since it causes issues because names are not standardized between samples
facsdata=as.data.frame(scale(as.matrix(facsdata),scale=TRUE,center=TRUE))#zscore by marker
facsdata=as.data.frame(apply(facsdata,2, minmax_normalize))#minmax normalize
rownames(expected_values)=toupper(rownames(expected_values))#Writes in upper case to make sure there is no mistake
colnames(facsdata)=toupper(colnames(facsdata))#same


intersect_markers=intersect(colnames(facsdata),rownames(expected_values))#Get common markers
intersect_markers=intersect_markers[order(intersect_markers)]#alphabetical order so there is no mistake
expected_values=expected_values[intersect_markers,]#ordering
facsdata=facsdata[,intersect_markers]#ordering
correlation_table=t(apply(facsdata,1,compute_corr,expected_values))#apply and store correlations by applying compute_corr function
assignments=t(apply(correlation_table,1,function(x){names(sort(x, decreasing=T))}))#get the list from the most resembling phenotype to the least resembling phenotype

dataf <- as.data.frame(correlation_table)
dataf$assigned <- factor(assignments[,1])
correlation_table[3,assignments[3,c(1,2)]]


define_new_cell_type<-function(data_vector,threshold){
  if(data_vector["delta"]<threshold){
    return(paste(data_vector["assigned"],data_vector["second"],sep="_"))
  }else{
    return("None")
  }
  }

replace.celltype <- function(data_vector){
  if(data_vector["new_cell_type"]!="None"){
    data_vector["new_correlation"] <- data_vector["new_cell_type"]
    data_vector <- data_vector[-length(data_vector)]
  }else{
  data_vector=data_vector[-length(data_vector)]}
}

add.new_correlation <- function(data_vector,threshold){
  if(data_vector["delta"]<threshold){
    data_vector<-as.numeric(data_vector)
    data_vector["new_celltype_correlation"] <- max(data_vector[-c(length(data_vector)-4:length(data_vector))])-(as.numeric(data_vector["delta"])/2)
  }else{
  data_vector["new_celltype_correlation"] <- data_vector[as.character(data_vector[["assigned"]])]}
}

#t.add.new_correlation <- function(data_vector,threshold){
#  if(data_vector["delta"]<threshold){
#    data_vector<-as.numeric(data_vector[1:(ncol(data_vector)-4)])
#}}


add_new_cell_type <- function(dataf,assignments,threshold=0.1,replace=FALSE){
  data=cbind(dataf,assignments[,2])
  colnames(data)[ncol(data)]<-"second"
  delta=apply(data,1,function(x){return(as.numeric(x[as.character(x["assigned"])])-as.numeric(x[as.character(x["second"])]))})
  data=cbind(data,delta)
  new_cell_type=apply(data,1,define_new_cell_type,threshold)
  res=cbind(data,new_cell_type)
  if(replace==TRUE){
    res=apply(res,1,replace.celltype)
  }
  #res=apply(res,1,add.new_correlation,threshold)
  return(as.data.frame(res))
  }

data=add_new_cell_type(dataf,assignments,threshold = 0.05,replace=FALSE)

data$batch=factor(unlist(lapply(rownames(data),function(x){unlist(str_split(x,"[.]"))[1]})))
data$cell=factor(unlist(lapply(rownames(data),function(x){unlist(str_split(x,"[.]"))[2]})))

mata=melt(data,id.vars=c("assigned","batch","cell"),measured.vars=colnames(data[,-c(ncol(data),ncol(data)-1,ncol(data)-2)]))
colnames(mata)<-c("Assigned","Batch","cell","cell_type","correlation_value")

mataf <- melt(dataf)
colnames(mataf) <- c("assigned","cell_type","correlation_value")

decreasingdelta <- t(apply(dataf[,colnames(dataf) != "assigned"],1,sort,decreasing=T))

dondtw <- as.data.frame(lapply(c(1:(ncol(decreasingdelta)-1)),function(x){decreasingdelta[,x]-decreasingdelta[,x+1]}))
colnames(dondtw) <- lapply(c(1:(ncol(decreasingdelta)-1)),function(x){c(x,x+1)})
montw <- melt(dondtw)
colnames(montw) <- c("delta","value")

dataf$batch=factor(unlist(lapply(rownames(dataf),function(x){unlist(str_split(x,"[.]"))[1]})))
dataf$cell_id=factor(unlist(lapply(rownames(dataf),function(x){unlist(str_split(x,"[.]"))[2]})))

```




