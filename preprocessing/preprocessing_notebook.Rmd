
```{r,include=FALSE}
require("flowCore") #Used for reading the data
require("ggplot2")
require("ggridges") #visualization
require("stringr") #set of functions to manipulate strings type in order to have nice titles
require("rlist") #set of functions that allows to easely manipulate lists
require("reshape2") #visualization
require("flowStats") #Alignment functions
require("scales") #scale colour intensity for visualization
require("dplyr")

#libraries
library("flowCore")

# installations
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("flowCore")

if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("flowStats")


plotdensity_flowset <- function(flowset){ ggplot(melt(lapply(as.list(flowset@frames),function(x){x=as.data.frame(x@exprs)})), aes(x=value,y=L1,fill=L1)) + geom_density_ridges(alpha=.4,verbose=FALSE) +facet_wrap(~variable)+theme_light()} #defines a function for visualizing flowset with densityplots
rename_markers<-function(flowset){#Defines a function to use marker names 
  copy_flowset=flowset[seq(along=flowset)]
  for (i in 1:length(copy_flowset)){
    marker.names=copy_flowset[[i]]@parameters@data$desc
    marker.names=lapply(marker.names,function(x){str_replace_all(x,"-","_")})
    colnames(copy_flowset[[i]]@exprs)<-unlist(lapply(marker.names, function(x){sapply(str_split(x,"_"),head,1)})) 
  }
  return(copy_flowset)
}

```

#Set user variables - This chunck allows the user to set the absolute path of the input data and the desired output folder.

--The input path has to contain only FCS files with or without an annotation file containing metadata. 

--The output path will be used for all the outputs (graphs and tables). The variable write_fcs_files has to be set to TRUE for writing fcs files at each step of the workflow (transformed and aligned), a folder for each step is created inside the output folder.

```{r}
#input_path <- "/home/alx/Documents/McGill_Neuro/Datasets/FACS_Paper/cell_line/" #Input folder containing multiple fcs files
input_path <- "/Users/rhalenathomas/GITHUB/PhenoID_single_cell_flow_cytometry_analysis/cell_line"
#output_path <- "/home/alx/Documents/McGill_Neuro/Datasets/FACS_Paper/output_test/" #Output path 
output_path <- "/Users/rhalenathomas/Documents/Projects_Papers/PhenoID/analysis/prepro_cell_lines"

write_fcs_files <- TRUE #Set to true to write fcs files at each step (subsampled, transformed, aligned and scaled) - recommanded 

#Create output folder if it's not already created
if(dir.exists(output_path)==FALSE){ #check
  dir.create(output_path) #create directory
}
```


#Read in the data - This chunck reads in the data and selects the area values that are used for analysis. It creates a flowSet object (containing multiple flowFrames, corresponding to fcs files)

```{r}
# note - wit would be good to make one function that reads in the data, selects the area values and indeed Alex has created such a function below 

flowset <- read.flowSet(path=input_path,transformation = FALSE ,emptyValue = FALSE,package="flowCore") #Create a flowset object


flowset <- fsApply(flowset,function(x){x[ ,grepl("^[{'FJComp'}|{'FCS'}|{'SSC'}].*A$",colnames(flowset))]}) #Apply a function to flowSet (fsApply) that selects only the columns corresponding to areas values using "regular expression".

# what do we need these tables for??? 
write.table(fsApply(flowset,function(x){dim(x@exprs)})[,1],file=paste0(output_path,"number_cells.csv"))#Counts the number of cells inside each flowframe and writes it into the output folder.

write.table(apply(as.data.frame(list.rbind(lapply(sampleNames(flowset),function(name){flowset[[name]]@description}))),2,as.character),file=paste0(output_path,"flowset_description.csv"))#Writes descriptions of each flowframe in a csv file

sampleNames(flowset) #Prints the name of each flowframe inside the flowset. User can modify it in the following chunck
#write.flowSet(flowset,outdir=paste0(output_path,"input_flowset"))#Writes the input flowset
#flowset=read.flowSet(path=paste0(output_path,"input_flowset"),phenoData = "annotation.txt")#Read the written flowset
```

#This chunck can be used for renaming the fcs files inside the flowset
```{r}
# create a vector with the new file names and assign them
# this must be a flowCore function
sampleNames(flowset) <- c("Astro03_06","Neuro03_06","NPC03_06","Oligo03_06","Astro03_17","IPSC03_17","NPC03_17","Oligo03_17") #directly modify the names inside the flowset
sampleNames(flowset)
# now the sample or file names are changed under the 'frame' data slot


# this appears to be again reading in the data an getting the expression values
# I'll make this a function for the package

# call this function 'make_flowset'
read_reg <- function(input_path,regular_expression,desc=FALSE) {
  flowset <- read.flowSet(path=input_path,transformation = FALSE ,emptyValue = FALSE,package="flowCore") #Create a flowset object
  if (desc==FALSE){ #Create a flowset object
    flowset <- fsApply(flowset,function(x){
      x[ ,grepl(regular_expression,colnames(flowset))]
      })
  }
  else{
    flowset <- fsApply(flowset,function(x){
      x[ ,which(grepl(regular_expression,x@parameters$desc))]
      })
  }
  return(flowset)
}


#flowset2 <- read_reg(input_path = input_path,"FJComp",desc=FALSE)
flowset2 <- make_flowset(input_path = input_path,"FJComp",desc=FALSE)

sampleNames(flowset2) <- c("Astro-06","Neuro-06","NPC-06","Oligo-06","Astro-17","IPSC-17","NPC-17","Oligo-17")

```

#Chunck used for loading organoids data (different chunck due to the names of the channels : "livedead","live dead")
```{r}
#Set the path of the file from the working directory
data_path="/home/alx/Documents/McGill_Neuro/Datasets/FACS_Paper/FIG2-A" 

#make a flowset object
exp=read.flowSet(path=data_path,transformation =FALSE ,emptyValue = FALSE,package="flowWorkspace")
# emptyValue = False doesn't consider empty rows

#select compensated channels, FCS and SSC that finishes by A (indicating the aea value) using regular expression
exp=fsApply(exp,function(x){x[ ,grepl("^[{'FJComp'}|{'FCS'}|{'SSC'}].*A$",colnames(exp))]})
#Still a flowset (fsApply -> apply a function to a flowset)

# if the data is located in a different space or you don't have the same type of fcs file exported (not complete) 
# don't run if the data is all the same folder and has all measures

data_path="/home/alx/Documents/McGill_Neuro/Datasets/FACS_Paper/FIG2"
exp1=read.flowSet(path=data_path,transformation =FALSE ,emptyValue = FALSE,package="flowWorkspace")
exp1=fsApply(exp1,function(x){x[ ,grepl("^[{'FJComp'}|{'FCS'}|{'SSC'}].*A$",colnames(exp1))]})#SELECT COMPENSED CHANNELS AND AEA VALUES
flowset=rbind2(exp,exp1) #FlowSet object
```

#Rename files
```{r}
sampleNames(flowset)<-c("3450c2","AIW2","AJG2","3450c1","3450c3","AIW1","AIW3","AJG1","AJG3") #directly modify the names inside the flowset
sampleNames(flowset)
```

#Subsets - Allows to subset each fcs with a desired number of cells selected randomly (set the seed), go directly to transformation step for not subsetting

Set an arbitrary number of cells for subsetting samples (if a fcs files has less cells it will be completely selected)
```{r,include=FALSE}
desired_size <- 3000
```
Set the number to the smallest fcs files (containing the smallest amount of cells)
```{r,include=FALSE}
desired_size <- min(fsApply(flowset,function(x){nrow(x@exprs)}))
```
Subsets the data
```{r,echo=FALSE}
set.seed(42) #Set a seed for reproducibility 
sf <- sampleFilter(filterId = "SizeFilter", size =desired_size) #Creates a "filter" object to subset
flowset <- fsApply(flowset,function(x){Subset(x,sf)}) #apply the filter on every flowframe

if(write_fcs_files==TRUE){
  write.flowSet(flowset,outdir=paste0(output_path,"subsample_input_flowset"))#Writes the subsample input flowset
  flowset=read.flowSet(path=paste0(output_path,"subsample_input_flowset"),phenoData = "annotation.txt")#Read the written flowset // The reason is for the phenodata management in the following steps
}
```

#Transformation -  Apply a mathematical function to make the data interpretable.
See [Finak G, Perez JM, Weng A, Gottardo R. Optimizing transformations for automated, high throughput analysis of flow cytometry data. BMC Bioinformatics. 2010;11:546. Published 2010 Nov 4. doi:10.1186/1471-2105-11-546] for more informations about transformations and possible optimizations.

See https://rdrr.io/bioc/flowCore/man/ for informations about available transformations in flowCore (used in this workflow)

Biexponential transformation
```{r,echo=FALSE}
inversebiexponentialTransform <- function(flowset,a = 0.5, b = 1, c = 0.5, d = 1, f = 0, w = 0){
  copy_flowset=flowset[seq(along=flowset)] #makes a copy of the input flowset
  for (i in 1:length(copy_flowset)){ #loop though index to get each flowframe
    copy_flowset[[i]]@exprs=a*exp(b*(copy_flowset[[i]]@exprs-w))-c*exp(-d*(copy_flowset[[i]]@exprs-w))+f
  }
  return(copy_flowset)
}

biexp  <- biexponentialTransform("biexp transform",a = 0.5, b = 1, c = 0.5, d = 1, f = 0, w = 0) #creates the transformation object (the values make it equivalent to arcsinh transform)
transformed_flowset <- transform(flowset, transformList(colnames(flowset), biexp)) #Apply the transformation

if(write_fcs_files==TRUE){#Check if user set the conditional for writing several fcs files
  write.flowSet(transformed_flowset,outdir=paste0(output_path,"transformed_flowset"))#writes the flowset
  transformed_flowset=read.flowSet(path=paste0(output_path,"transformed_flowset"),phenoData = "annotation.txt")
}
```


Scaling looses informations about the data and makes it harder to align. The main interest of this step is to harmonize the data for the classifier and make it comparable from one experience to another, thus I suggest to apply this step after the clustering.
We could potentially makes it after the alignment but since every flow frame (.fcs) is different from each other we would loose the alignment. (it would be interesting to try to normalize each marker considering all experiments at the same time)

Moreover the actual presence of some outlayers (supposively from a bad cleaning of the data in the first place) biase the results concerning the z-score normalization.

#Alignment

Alignment of organoids
```{r}
normtr=gaussNorm(transformed_flowset,colnames(transformed_flowset)[c(3,5:6,9:length(colnames(transformed_flowset)))],max.lms = 2,peak.density.thr = 0.01) #Detects and align 2 peaks on the marker 3,5,6,9...14. 
expbe_norm2=normtr$flowset
normtr=gaussNorm(expbe_norm2,colnames(expbe_norm2)[c(4,7:8)],max.lms = 1,peak.density.thr = 0.05)#Detects and align 1 peak 
aligned_transformed_flowset=normtr$flowset
retrotransformed_flowset <- inversebiexponentialTransform(aligned_transformed_flowset) #apply the function for cancelling biexp transform 
```

Be carefull with this step. Check the density plots before and after the alignment to make sure peaks are aligned
```{r}
normtr <- gaussNorm(flowset = transformed_flowset,channel.names = colnames(transformed_flowset)[c(3:length(colnames(transformed_flowset)))], max.lms = 2, peak.density.thr = 0.01, peak.distance.thr = 0.5) #Detects and align 2 peaks (max.lms) for all markers except FSC and SSC (columns 3 to number of markers) the threshold are data-dependant and may have to differ from one analysis to another.

aligned_transformed_flowset <- normtr$flowset #Extract the flowset from the result of the alignment
retrotransformed_flowset <- inversebiexponentialTransform(aligned_transformed_flowset) #apply the function for cancelling biexp transform
```



```{r}
plotdensity_flowset(aligned_transformed_flowset)
plotdensity_flowset(transformed_flowset)
```

Alignment of 2D cell cultures

```{r}
flowAstro=transformed_flowset[c("Astro03_06","Astro03_17"),] #defines multiple flowset for aligning data that comes from the same strain
flowOligo=transformed_flowset[c("Oligo03_06","Oligo03_17"),]
flowNPC=transformed_flowset[c("NPC03_06","NPC03_17"),]

#align 1 peak of Astro
alignflowAstro=gaussNorm(flowAstro,channel.names = colnames(flowAstro)[c(4,7,8,13)], max.lms = 1, peak.density.thr = 0.01, peak.distance.thr = 0.5)

#align 2 peaks of Astro
alignflowAstro=gaussNorm(alignflowAstro$flowset,channel.names = colnames(flowAstro)[c(3,5,6,9,10,11,12,14,15)], max.lms = 2, peak.density.thr = 0.01, peak.distance.thr = 0.5)


#align 1 peak of Oligo
alignflowOligo=gaussNorm(flowOligo,channel.names = colnames(flowOligo)[c(4,7,11)], max.lms = 1, peak.density.thr = 0.01, peak.distance.thr = 0.5)

#align 2 peaks of Oligo
alignflowOligo=gaussNorm(alignflowOligo$flowset,channel.names = colnames(flowOligo)[c(3,5,6,9:10,12,13,14)], max.lms = 2, peak.density.thr = 0.01, peak.distance.thr = 0.5)


#align 1 peak of NPC
alignflowNPC=gaussNorm(flowNPC,channel.names = colnames(flowNPC)[c(4,11)], max.lms = 1, peak.density.thr = 0.01, peak.distance.thr = 0.5)

#align 2 peaks of NPC
alignflowNPC=gaussNorm(alignflowNPC$flowset,channel.names = colnames(flowNPC)[c(3,5,6,7,10,13,14,15,9)], max.lms = 2, peak.density.thr = 0.01, peak.distance.thr = 0.5)


plotdensity_flowset(flowAstro)
plotdensity_flowset(alignflowAstro$flowset)

plotdensity_flowset(flowOligo)
plotdensity_flowset(alignflowOligo$flowset)

plotdensity_flowset(flowNPC)
plotdensity_flowset(alignflowNPC$flowset)

aligned_transformed_flowset=flowSet(c(transformed_flowset@frames$IPSC03_17,
                                      transformed_flowset@frames$Neuro03_06,
                                      alignflowAstro$flowset@frames$Astro03_06,
                                      alignflowAstro$flowset@frames$Astro03_17,
                                      alignflowNPC$flowset@frames$NPC03_06,
                                      alignflowNPC$flowset@frames$NPC03_17,
                                      alignflowOligo$flowset@frames$Oligo03_06,
                                      alignflowOligo$flowset@frames$Oligo03_17))
sampleNames(aligned_transformed_flowset) <- c("IPSC03_17","Neuro03_06","Astro03_06","Astro03_17","NPC03_06","NPC03_17","Oligo03_06","Oligo03_17")
retrotransformed_flowset <- inversebiexponentialTransform(aligned_transformed_flowset) #apply the function for cancelling biexp transform
```


Densityplots for showing effects of transformations and alignment
```{r}
plotdensity_flowset(rename_markers(flowset))
plotdensity_flowset(rename_markers(transformed_flowset))
plotdensity_flowset(rename_markers(aligned_transformed_flowset))
plotdensity_flowset(rename_markers(retrotransformed_flowset))
```


#Writing csv files with informations

```{r}
flowset_to_csv=function(flowset){  
  list_of_flowframes=fsApply(rename_markers(flowset),function(x){as.data.frame(x@exprs)})#Makes a list of dataframes
  list_names=names(list_of_flowframes)#extract names for not calling names() function at each loop
  for (index in seq_along(list_of_flowframes)){ #Iterates along the index for adding sample names
    list_of_flowframes[[index]]=list_of_flowframes[[index]] %>% #Using tidyverse package for adding features (name of batch)
      mutate(Batch=list_names[index])
  }
  ds=list.rbind(list_of_flowframes)#binds every fcs file in a single dataframe
  ds$cell=as.factor(unlist(lapply(as.list(c(1:length(flowset))),function(x){c(1:nrow(flowset[[x]]@exprs))})))#add cell IDs - cell count per sample 
  write.csv(ds,file=paste0(output_path,deparse(substitute(flowset)),".csv"))#save the R data for further usage
}

flowset_to_csv(flowset)#apply the function
flowset_to_csv(transformed_flowset)#apply the function
flowset_to_csv(aligned_transformed_flowset)#apply the function
flowset_to_csv(retrotransformed_flowset)#apply the function


#Writes a text file with more informations about the system
sys=Sys.info() #store informations about the system
fileConn<-file(paste0(output_path,"call_description.txt")) #Writes txt files that describe the work 
writeLines(c(paste0(as.character(Sys.time())," ",Sys.timezone()),
            paste0("Import ", "8"," fcs files :"),
            "",
            sampleNames(flowset),
            "",
            paste0("data is subset to ",as.character(desired_size)," cells per fcs file"),
            "",
            paste0("written in ", output_path),
            "",
            "system informations:",
            paste0("sysname : ",sys[1]),
            paste0("release : ",sys[2]),
            paste0("version : ",sys[3]),
            paste0("node name : ",sys[4]),
            paste0("machine : ",sys[5]),
            paste0("login : ",sys[6]),
            paste0("user : ",sys[7]),
            paste0("effective_user : ",sys[8])
            ),
          fileConn)
close(fileConn)

```

/!\ I kept normalization steps (scaling) in the notebook but I don't recommand to use it on cell lines. 

#Scaling - outlayers make the zscore normalization messy, it is not the case on "clean" dataset such as organoids. 
```{r}
zscorenorm_flowset <- function(flowset){ #Defines a function for applying zscore normalization to each flowframe within the flowset in parameter
  copy_flowset=flowset[seq(along=flowset)] #makes a copy of the input flowset
  for (i in 1:length(copy_flowset)){ #loop though index to get each flowframe
    copy_flowset[[i]]@exprs=scale(flowset[[i]]@exprs,center=TRUE,scale=TRUE)#zscore using the default scale function
  }
  return(copy_flowset)#Return the scaled flowset
}

minmax_flowset <- function(flowset){ #Defines a function for applying zscore normalization to each flowframe within the flowset in parameter
  copy_flowset=flowset[seq(along=flowset)] #makes a copy of the input flowset
  for (i in 1:length(copy_flowset)){ #loop though index to get each flowframe
    copy_flowset[[i]]@exprs=apply(flowset[[i]]@exprs,2,function(x){(x-min(x))/(max(x)-min(x))})#minmax normalization
  }
  return(copy_flowset)#Return the scaled flowset
}

scaledminmax_transformed_flowset <- minmax_flowset(aligned_transformed_flowset)
scaledzscore_transformed_flowset <- zscorenorm_flowset(transformed_flowset)

plotdensity_flowset(scaledminmax_transformed_flowset)
plotdensity_flowset(scaledzscore_transformed_flowset)
```


#This chunck allows the user to write in csv mean values of a given flowset per marker
```{r}
list_of_flowframes=fsApply(rename_markers(transformed_flowset),function(x){as.data.frame(x@exprs[,-c(1,2,9)])})#Makes a list of dataframes without FSC SSC and LiveDead markers
mean_values=as.data.frame(lapply(list_of_flowframes,function(x){apply(x,2,function(y){mean(y)})}))#compute the mean by marker within each element of the list of flowframes (dataframes)
write.csv(mean_values,file="/home/alx/Desktop/transformed_mean_values_2D_cell_lines.csv")
```

#Correlation analysis

```{r}
expected_val=read.csv(file="/home/alx/Downloads/ReferenceMatrix_meanof6_20210517_B.csv") 
#Format expected values to fit the old one
expected_val=t(expected_val) #transpose the dataframe for fitting the previous way of making correlation
colnames(expected_val) <- expected_val[1,]#give column names the right names

expected_values=apply(as.data.frame(expected_val[-1,]),2, function(x){as.numeric(as.character(x))}) #change character to numeric values in a tortured way
rownames(expected_values)=rownames(expected_val[-1,]) #right rownames
expected_values=expected_values[,colnames(expected_values)!="Pericyte" & colnames(expected_values) != "Microglial"] #get rid of microglia and pericytes
```

```{r}

output_path="/home/alx/Documents/McGill_Neuro/Scripts/PhenoID/output_june/"
#2 functions to help
minmax_normalize <- function(x, na.rm=TRUE){return((x- min(x[!is.na(x)])) /(max(x[!is.na(x)])-min(x[!is.na(x)])))}
expected_values <- minmax_normalize(scale(expected_values)) #apply zscore normalization and minmax normalization (fit results between 0 and 1)

compute_corr<-function(cell_profile,celltypes_profile){
  celltypes=celltypes_profile[,c(1:ncol(celltypes_profile))]#2 because the first column of the given file is expected to be "annotation"
  res=as.vector(apply(celltypes,2,function(x){
    cor(cell_profile[which(!is.na(x))],x[which(!is.na(x))])}))#If a marker is "na" it won't be considered for computing the correlation coef
  names(res)<-colnames(celltypes)
  return(res)
}

fs=rename_markers(aligned_transformed_flowset) #Nice marker names

facsdata=list.rbind(lapply(as.list(fs@frames),function(x){x=as.data.frame(x@exprs)})) #I had to remove the live-dead since it causes issues because names are not standardized between samples
facsdata=as.data.frame(scale(as.matrix(facsdata),scale=TRUE,center=TRUE))#zscore by marker
facsdata=as.data.frame(apply(facsdata,2, minmax_normalize))#minmax normalize
rownames(expected_values)=toupper(rownames(expected_values))#Writes in upper case to make sure there is no mistake
colnames(facsdata)=toupper(colnames(facsdata))#same


intersect_markers=intersect(colnames(facsdata),rownames(expected_values))#Get common markers
intersect_markers=intersect_markers[order(intersect_markers)]#alphabetical order so there is no mistake
expected_values=expected_values[intersect_markers,]#ordering
facsdata=facsdata[,intersect_markers]#ordering
correlation_table=t(apply(facsdata,1,compute_corr,expected_values))#apply and store correlations by applying compute_corr function
assignments=t(apply(correlation_table,1,function(x){names(sort(x, decreasing=T))}))#get the list from the most resembling phenotype to the least resembling phenotype

dataf <- as.data.frame(correlation_table)
dataf$assigned <- factor(assignments[,1])
correlation_table[3,assignments[3,c(1,2)]]


define_new_cell_type<-function(data_vector,threshold){
  if(data_vector["delta"]<threshold){
    return(paste(data_vector["assigned"],data_vector["second"],sep="_"))
  }else{
    return("None")
  }
  }

replace.celltype <- function(data_vector){
  if(data_vector["new_cell_type"]!="None"){
    data_vector["new_correlation"] <- data_vector["new_cell_type"]
    data_vector <- data_vector[-length(data_vector)]
  }else{
  data_vector=data_vector[-length(data_vector)]}
}

add.new_correlation <- function(data_vector,threshold){
  if(data_vector["delta"]<threshold){
    data_vector<-as.numeric(data_vector)
    data_vector["new_celltype_correlation"] <- max(data_vector[-c(length(data_vector)-4:length(data_vector))])-(as.numeric(data_vector["delta"])/2)
  }else{
  data_vector["new_celltype_correlation"] <- data_vector[as.character(data_vector[["assigned"]])]}
}

#t.add.new_correlation <- function(data_vector,threshold){
#  if(data_vector["delta"]<threshold){
#    data_vector<-as.numeric(data_vector[1:(ncol(data_vector)-4)])
#}}


add_new_cell_type <- function(dataf,assignments,threshold=0.1,replace=FALSE){
  data=cbind(dataf,assignments[,2])
  colnames(data)[ncol(data)]<-"second"
  delta=apply(data,1,function(x){return(as.numeric(x[as.character(x["assigned"])])-as.numeric(x[as.character(x["second"])]))})
  data=cbind(data,delta)
  new_cell_type=apply(data,1,define_new_cell_type,threshold)
  res=cbind(data,new_cell_type)
  if(replace==TRUE){
    res=apply(res,1,replace.celltype)
  }
  #res=apply(res,1,add.new_correlation,threshold)
  return(as.data.frame(res))
  }

data=add_new_cell_type(dataf,assignments,threshold = 0.05,replace=FALSE)

data$batch=factor(unlist(lapply(rownames(data),function(x){unlist(str_split(x,"[.]"))[1]})))
data$cell=factor(unlist(lapply(rownames(data),function(x){unlist(str_split(x,"[.]"))[2]})))

mata=melt(data,id.vars=c("assigned","batch","cell"),measured.vars=colnames(data[,-c(ncol(data),ncol(data)-1,ncol(data)-2)]))
colnames(mata)<-c("Assigned","Batch","cell","cell_type","correlation_value")

mataf <- melt(dataf)
colnames(mataf) <- c("assigned","cell_type","correlation_value")

decreasingdelta <- t(apply(dataf[,colnames(dataf) != "assigned"],1,sort,decreasing=T))

dondtw <- as.data.frame(lapply(c(1:(ncol(decreasingdelta)-1)),function(x){decreasingdelta[,x]-decreasingdelta[,x+1]}))
colnames(dondtw) <- lapply(c(1:(ncol(decreasingdelta)-1)),function(x){c(x,x+1)})
montw <- melt(dondtw)
colnames(montw) <- c("delta","value")

dataf$batch=factor(unlist(lapply(rownames(dataf),function(x){unlist(str_split(x,"[.]"))[1]})))
dataf$cell_id=factor(unlist(lapply(rownames(dataf),function(x){unlist(str_split(x,"[.]"))[2]})))

```




